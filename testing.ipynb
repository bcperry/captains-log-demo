{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a34b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from os import path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "# Load environment variables from .azure/captainslog/.env\n",
    "load_dotenv('.azure/captainslog/.env')\n",
    "file = r\"C:\\Users\\blaineperry\\Downloads\\output.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1263cac3",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Key: CO34dRMoGfb3pQ6xBo3pxUV2KUHPZce7Vgs5xkmWCuQDRtojCwZTJQQJ99BFAJhseKSIUj3iAAAYACOGOm6s\n",
      "Speech Region: usgovvirginia\n",
      "Speech Endpoint: https://cog-speechg5azofxckevgi.cognitiveservices.azure.us/\n",
      "404\n",
      "{'error': {'code': '404', 'message': 'Resource not found'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Get the environment variables\n",
    "speech_key = os.getenv('AZURE_SPEECH_KEY')\n",
    "speech_region = os.getenv('AZURE_SPEECH_REGION')\n",
    "speech_endpoint = os.getenv('AZURE_SPEECH_ENDPOINT')\n",
    "\n",
    "stt_api = 'api.cognitive.microsoft.us'\n",
    "\n",
    "# Print the environment variables to verify they are loaded correctly\n",
    "print(f\"Speech Key: {speech_key}\")\n",
    "print(f\"Speech Region: {speech_region}\")\n",
    "print(f\"Speech Endpoint: {speech_endpoint}\")\n",
    "\n",
    "# Define the endpoint URL\n",
    "url = f\"https://{speech_region}.{stt_api}/speech/recognition/conversation/cognitiveservices/v1\"\n",
    "\n",
    "# Define the parameters\n",
    "params = {\n",
    "    'language': 'en-US',\n",
    "    'format': 'detailed'\n",
    "}\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': speech_key,\n",
    "    'Content-Type': 'audio/wav'\n",
    "}\n",
    "\n",
    "# Read the audio file\n",
    "with open(file, 'rb') as audio_file:\n",
    "    audio_data = audio_file.read()\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, params=params, headers=headers, data=audio_data)\n",
    "\n",
    "# Print the response\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210a291c",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "usGovEndpoint = \"wss://usgovvirginia.stt.speech.azure.us\"\n",
    "speech_config = speechsdk.SpeechConfig(endpoint=usGovEndpoint, subscription=speech_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6413c293",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def recognize_from_microphone():\n",
    "     # This example requires environment variables named \"SPEECH_KEY\" and \"ENDPOINT\"\n",
    "     # Replace with your own subscription key and endpoint, the endpoint is like : \"https://YourServiceRegion.api.cognitive.microsoft.com\"\n",
    "    speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and endpoint values?\")\n",
    "\n",
    "# recognize_from_microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241a19f",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8359587",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "\n",
    "def split_audio(input_file):\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "\n",
    "    # Define the chunk length (e.g., 30 seconds)\n",
    "    chunk_duration_ms = 30 * 1000 # in milliseconds\n",
    "\n",
    "    # Calculate number of chunks\n",
    "    num_chunks = (len(audio) // chunk_duration_ms)+1\n",
    "\n",
    "    # Split the audio file into chunks\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_duration_ms\n",
    "        end = (i + 1) * chunk_duration_ms\n",
    "        chunk = audio[start:end]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def transcribe_working(filename: str):\n",
    "    # Split audio file into chunks\n",
    "    audio_chunks = split_audio(filename)\n",
    "    print(\"Number of audio chunks: {}\".format(len(audio_chunks)))\n",
    "\n",
    "    full_transcription = \"\"\n",
    "    # Transcribe each chunk\n",
    "    for chunk in audio_chunks:\n",
    "\n",
    "        # Create a temporary file to store the audio chunk\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_audio_file:\n",
    "            chunk.export(temp_audio_file.name, format=\"wav\")\n",
    "            # print(f\"Temporary file created: {temp_audio_file.name}\")\n",
    "            # print(f\"Chunk duration: {len(chunk)} ms\")\n",
    "            # print(f\"Chunk format: WAV\")\n",
    "            audio_config = speechsdk.audio.AudioConfig(filename=temp_audio_file.name)\n",
    "\n",
    "            speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "            \n",
    "            # with open(temp_audio_file.name, \"rb\") as audio_file:\n",
    "            transcription = speech_recognizer.recognize_once()\n",
    "            # print(transcription.json)\n",
    "        # return transcription\n",
    "            if isinstance(transcription, dict):\n",
    "                text = transcription['text']\n",
    "            else:\n",
    "                text = transcription.text\n",
    "            print(text)\n",
    "            full_transcription = full_transcription + text\n",
    "            \n",
    "        # Close and Delete the temporary audio file\n",
    "        temp_audio_file.close()\n",
    "        # os.unlink(temp_audio_file.name)\n",
    "    return full_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6529474e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audio chunks: 70\n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: \n",
      "full_transcription: Talk to they, they want to go all sorts of different routes and some of them, I don't know if they're good ideas, but I'm not knowledgeable enough to tell them you don't want to do that. So we're not telling anybody no yet, but we're seeing people want to pull in all kinds of different tools and maybe that's fine if they're just their heart set on it, but.\n",
      "full_transcription: Talk to they, they want to go all sorts of different routes and some of them, I don't know if they're good ideas, but I'm not knowledgeable enough to tell them you don't want to do that. So we're not telling anybody no yet, but we're seeing people want to pull in all kinds of different tools and maybe that's fine if they're just their heart set on it, but.Case in point, are you familiar with Camo GPT folks that are just like, we want camo GPT? It's like, OK, but I don't think it's an available LLM in the at least not yet. Yeah. So Camo GPT is an interesting one, right, Because the Artificial Intelligence Integration Center up in Pittsburgh, these were kind of my people, I spent a couple years up there with them as well. They are looking to migrate all of their stuff to the Azure services, right? So specifically Azure opening.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m file = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mblaineperry\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDownloads\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtest.mp3\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# transcribe(file)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m transcription = \u001b[43mtranscribe_working\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m transcription\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mtranscribe_working\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m     39\u001b[39m     speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# with open(temp_audio_file.name, \"rb\") as audio_file:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     transcription = \u001b[43mspeech_recognizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecognize_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# print(transcription.json)\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# return transcription\u001b[39;00m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transcription, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blaineperry\\git\\captains-log-v2\\.venv\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:1156\u001b[39m, in \u001b[36mSpeechRecognizer.recognize_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecognize_once\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> SpeechRecognitionResult:\n\u001b[32m   1147\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1148\u001b[39m \u001b[33;03m    Performs recognition in a blocking (synchronous) mode. Returns after a single utterance is\u001b[39;00m\n\u001b[32m   1149\u001b[39m \u001b[33;03m    recognized. The end of a single utterance is determined by listening for silence at the end\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1154\u001b[39m \u001b[33;03m    :returns: The result value of the synchronous recognition.\u001b[39;00m\n\u001b[32m   1155\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecognize_once_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blaineperry\\git\\captains-log-v2\\.venv\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:612\u001b[39m, in \u001b[36mResultFuture.get\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    609\u001b[39m \u001b[33;03mWaits until the result is available, and returns it.\u001b[39;00m\n\u001b[32m    610\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__resolved:\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     result_handle = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__wrapped_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    614\u001b[39m         \u001b[38;5;28mself\u001b[39m.__result = \u001b[38;5;28mself\u001b[39m.__wrapped_type(result_handle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blaineperry\\git\\captains-log-v2\\.venv\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:1172\u001b[39m, in \u001b[36mSpeechRecognizer.recognize_once_async.<locals>.resolve_future\u001b[39m\u001b[34m(handle)\u001b[39m\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresolve_future\u001b[39m(handle: _spx_handle):\n\u001b[32m   1171\u001b[39m     result_handle = _spx_handle(\u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1172\u001b[39m     \u001b[43m_call_hr_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_sdk_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecognizer_recognize_once_async_wait_for\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_uint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_handle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m     _sdk_lib.recognizer_async_handle_release(handle)\n\u001b[32m   1174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result_handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\blaineperry\\git\\captains-log-v2\\.venv\\Lib\\site-packages\\azure\\cognitiveservices\\speech\\interop.py:61\u001b[39m, in \u001b[36m_call_hr_fn\u001b[39m\u001b[34m(fn, *args)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_hr_fn\u001b[39m(*args, fn):\n\u001b[32m     60\u001b[39m     fn.restype = _spx_hr\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     hr = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m fn()\n\u001b[32m     62\u001b[39m     _raise_if_failed(hr)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "file = r\"C:\\Users\\blaineperry\\Downloads\\output (1).wav\"\n",
    "# file = r\"C:\\Users\\blaineperry\\Downloads\\MSFT_OSS Azure AI Services discussion [In-person]-20250306_125903-Meeting Recording.mp4\"\n",
    "file = r\"C:\\Users\\BLAINE~1\\AppData\\Local\\Temp\\tmpqt2h_ugu.wav\"\n",
    "file = r\"C:\\Users\\blaineperry\\Downloads\\test.mp3\"\n",
    "# transcribe(file)\n",
    "transcription = transcribe_working(file)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c52951",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "print(transciption.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01814d",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98bf6e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# import required modules\n",
    "\n",
    "\n",
    "# assign files\n",
    "filename = r\"C:\\Users\\blaineperry\\Downloads\\test.mp3\"\n",
    "filename = file\n",
    "output_file = \"result.wav\"\n",
    "\n",
    "# get the filetype from the input file text\n",
    "file_extension = path.splitext(filename)[1].lower()[1:]\n",
    "print(f\"Input file: {filename}, File extension: {file_extension}\")\n",
    "# convert mp3 file to wav file\n",
    "sound = AudioSegment.from_file(filename, file_extension)\n",
    "# Split audio into 30-second chunks\n",
    "chunk_duration_ms = 30 * 1000  # 30 seconds in milliseconds\n",
    "chunks = []\n",
    "\n",
    "for i in range(0, len(sound), chunk_duration_ms):\n",
    "    chunk = sound[i:i + chunk_duration_ms]\n",
    "    chunks.append(chunk)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks of 30 seconds each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c85fa",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
