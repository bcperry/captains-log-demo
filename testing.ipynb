{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a34b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from os import path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "# Load environment variables from .azure/captainslog/.env\n",
    "load_dotenv('.azure/captainslog/.env')\n",
    "file = r\"C:\\Users\\blaineperry\\Downloads\\output.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1263cac3",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Key: CO34dRMoGfb3pQ6xBo3pxUV2KUHPZce7Vgs5xkmWCuQDRtojCwZTJQQJ99BFAJhseKSIUj3iAAAYACOGOm6s\n",
      "Speech Region: usgovvirginia\n",
      "Speech Endpoint: https://cog-speechg5azofxckevgi.cognitiveservices.azure.us/\n",
      "404\n",
      "{'error': {'code': '404', 'message': 'Resource not found'}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Get the environment variables\n",
    "speech_key = os.getenv('AZURE_SPEECH_KEY')\n",
    "speech_region = os.getenv('AZURE_SPEECH_REGION')\n",
    "speech_endpoint = os.getenv('AZURE_SPEECH_ENDPOINT')\n",
    "\n",
    "stt_api = 'api.cognitive.microsoft.us'\n",
    "\n",
    "# Print the environment variables to verify they are loaded correctly\n",
    "print(f\"Speech Key: {speech_key}\")\n",
    "print(f\"Speech Region: {speech_region}\")\n",
    "print(f\"Speech Endpoint: {speech_endpoint}\")\n",
    "\n",
    "# Define the endpoint URL\n",
    "url = f\"https://{speech_region}.{stt_api}/speech/recognition/conversation/cognitiveservices/v1\"\n",
    "\n",
    "# Define the parameters\n",
    "params = {\n",
    "    'language': 'en-US',\n",
    "    'format': 'detailed'\n",
    "}\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': speech_key,\n",
    "    'Content-Type': 'audio/wav'\n",
    "}\n",
    "\n",
    "# Read the audio file\n",
    "with open(file, 'rb') as audio_file:\n",
    "    audio_data = audio_file.read()\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, params=params, headers=headers, data=audio_data)\n",
    "\n",
    "# Print the response\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210a291c",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "usGovEndpoint = \"wss://usgovvirginia.stt.speech.azure.us\"\n",
    "speech_config = speechsdk.SpeechConfig(endpoint=usGovEndpoint, subscription=speech_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413c293",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# def recognize_from_microphone():\n",
    "#      # This example requires environment variables named \"SPEECH_KEY\" and \"ENDPOINT\"\n",
    "#      # Replace with your own subscription key and endpoint, the endpoint is like : \"https://YourServiceRegion.api.cognitive.microsoft.com\"\n",
    "#     speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "#     audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "#     speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "#     print(\"Speak into your microphone.\")\n",
    "#     speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "#     if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "#         print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "#     elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "#         print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "#     elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "#         cancellation_details = speech_recognition_result.cancellation_details\n",
    "#         print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "#         if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "#             print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "#             print(\"Did you set the speech resource key and endpoint values?\")\n",
    "\n",
    "# # recognize_from_microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8359587",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "\n",
    "def split_audio(input_file):\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "\n",
    "    # Define the chunk length (e.g., 30 seconds)\n",
    "    chunk_duration_ms = 30 * 1000 # in milliseconds\n",
    "\n",
    "    # Calculate number of chunks\n",
    "    num_chunks = (len(audio) // chunk_duration_ms)+1\n",
    "\n",
    "    # Split the audio file into chunks\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_duration_ms\n",
    "        end = (i + 1) * chunk_duration_ms\n",
    "        chunk = audio[start:end]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def transcribe(filename: str):\n",
    "    # Split audio file into chunks\n",
    "    audio_chunks = split_audio(filename)\n",
    "    print(\"Number of audio chunks: {}\".format(len(audio_chunks)))\n",
    "    print(f\"Transcribing {len(audio_chunks)} audio chunks from {filename}\")\n",
    "    print(f\"Audio duration: {len(AudioSegment.from_file(filename)) / 1000} seconds\")\n",
    "\n",
    "    full_transcription = \"\"\n",
    "    # Transcribe each chunk\n",
    "    for chunk in audio_chunks:\n",
    "\n",
    "        # Create a temporary file to store the audio chunk\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_audio_file:\n",
    "            chunk.export(temp_audio_file.name, format=\"wav\")\n",
    "\n",
    "            audio_config = speechsdk.audio.AudioConfig(filename=temp_audio_file.name)\n",
    "            speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "            \n",
    "            transcription = speech_recognizer.recognize_once()\n",
    "            print(f\"Transcription for chunk: {transcription.json}\")\n",
    "\n",
    "        # return transcription\n",
    "            if isinstance(transcription, dict):\n",
    "                text = transcription['text']\n",
    "            else:\n",
    "                text = transcription.text\n",
    "            print(text)\n",
    "            full_transcription = full_transcription + text\n",
    "            \n",
    "        # Close and Delete the temporary audio file\n",
    "        temp_audio_file.close()\n",
    "        # os.unlink(temp_audio_file.name)\n",
    "    return full_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6529474e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audio chunks: 1\n",
      "Transcribing 1 audio chunks from C:\\Users\\blaineperry\\Downloads\\output (1).wav\n",
      "Audio duration: 1.238 seconds\n",
      "Transcription for chunk: {\"Id\":\"d3d91a5a6a8745fcb6f9ee8d76589cfb\",\"RecognitionStatus\":\"Success\",\"DisplayText\":\"Download me.\",\"Offset\":700000,\"Duration\":10900000,\"Channel\":0}\n",
      "Download me.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Download me.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = r\"C:\\Users\\blaineperry\\Downloads\\output (1).wav\"\n",
    "# file = r\"C:\\Users\\blaineperry\\Downloads\\MSFT_OSS Azure AI Services discussion [In-person]-20250306_125903-Meeting Recording.mp4\"\n",
    "# file = r\"C:\\Users\\BLAINE~1\\AppData\\Local\\Temp\\tmpqt2h_ugu.wav\"\n",
    "# file = r\"C:\\Users\\blaineperry\\Downloads\\test.mp3\"\n",
    "# transcribe(file)\n",
    "transcription = transcribe(file)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af98bf6e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: C:\\Users\\blaineperry\\Downloads\\output (1).wav, File extension: wav\n",
      "Created 1 chunks of 30 seconds each\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "\n",
    "\n",
    "# assign files\n",
    "filename = r\"C:\\Users\\blaineperry\\Downloads\\test.mp3\"\n",
    "filename = file\n",
    "output_file = \"result.wav\"\n",
    "\n",
    "# get the filetype from the input file text\n",
    "file_extension = path.splitext(filename)[1].lower()[1:]\n",
    "print(f\"Input file: {filename}, File extension: {file_extension}\")\n",
    "# convert mp3 file to wav file\n",
    "sound = AudioSegment.from_file(filename, file_extension)\n",
    "# Split audio into 30-second chunks\n",
    "chunk_duration_ms = 30 * 1000  # 30 seconds in milliseconds\n",
    "chunks = []\n",
    "\n",
    "for i in range(0, len(sound), chunk_duration_ms):\n",
    "    chunk = sound[i:i + chunk_duration_ms]\n",
    "    chunks.append(chunk)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks of 30 seconds each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757c85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file as a bytes IO object\n",
    "with open(file, 'rb') as f:\n",
    "    audio_bytes = f.read()\n",
    "\n",
    "# Create a BytesIO object from the audio data\n",
    "from io import BytesIO\n",
    "audio_io = BytesIO(audio_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a68a14a",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BytesIO at 0x1914b7c8fe0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cdd7a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio data written to temporary file: C:\\Users\\BLAINE~1\\AppData\\Local\\Temp\\tmpguyl17ei.wav\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "\n",
    "# Create a named temporary file and write the BytesIO object to it\n",
    "with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n",
    "    temp_file.write(audio_io.getvalue())\n",
    "    temp_filename = temp_file.name\n",
    "\n",
    "print(f\"Audio data written to temporary file: {temp_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe343a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
